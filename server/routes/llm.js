/**
 * LLM API Routes
 * Handles LLM requests and responses
 * Updated to support GPT-5.1 (released Nov 12, 2025)
 */

const express = require('express');
const router = express.Router();

// Default model - GPT-5.1 (released November 12, 2025)
const DEFAULT_MODEL = 'gpt-5.1';
const DEFAULT_MAX_TOKENS = 10000;

/**
 * @route   POST /api/chat/completions
 * @desc    OpenAI-compatible chat completions endpoint (supports GPT-5.1)
 * @access  Private
 */
router.post('/chat/completions', async (req, res) => {
  try {
    const { messages, model, temperature, max_tokens, stream } = req.body;
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
    const hasImages = messages?.some(m => 
      Array.isArray(m.content) && m.content.some(c => c.type === 'image_url')
    );
    
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º gpt-4o –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç vision), –∏–Ω–∞—á–µ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    let selectedModel = model || DEFAULT_MODEL;
    if (hasImages) {
      selectedModel = 'gpt-4o'; // gpt-4o –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç vision –∏ –ª—É—á—à–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä—É–∫–æ–ø–∏—Å–Ω—ã–π —Ç–µ–∫—Å—Ç
      console.log('üñºÔ∏è Images detected, switching to gpt-4o for vision support');
    }
    
    console.log('Chat completions request:', { 
      model: selectedModel,
      messagesCount: messages?.length,
      temperature,
      max_tokens: max_tokens || DEFAULT_MAX_TOKENS,
      stream,
      hasImages
    });

    if (!messages || !Array.isArray(messages) || messages.length === 0) {
      return res.status(400).json({ error: 'Missing or invalid messages array' });
    }

    // Get the last user message for context
    const lastUserMessage = messages.filter(m => m.role === 'user').pop();
    const systemMessage = messages.find(m => m.role === 'system');

    // Check if OpenAI API key is configured
    const openaiApiKey = process.env.OPENAI_API_KEY;

    if (openaiApiKey) {
      // Use real OpenAI API with GPT-5.1
      try {
        const requestBody = {
          model: selectedModel,
          messages: messages,
          temperature: temperature || 0.7,
          max_completion_tokens: max_tokens || DEFAULT_MAX_TOKENS,
          stream: stream || false
        };

        console.log(`üöÄ Calling OpenAI API with model: ${selectedModel}`);

        const response = await fetch('https://api.openai.com/v1/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${openaiApiKey}`
          },
          body: JSON.stringify(requestBody)
        });

        if (!response.ok) {
          const errorData = await response.text();
          console.error('OpenAI API error:', errorData);
          throw new Error(`OpenAI API error: ${response.status}`);
        }

        if (stream) {
          // Handle streaming response
          console.log('OpenAI streaming response started');
          res.setHeader('Content-Type', 'text/plain; charset=utf-8');
          res.setHeader('Cache-Control', 'no-cache');
          res.setHeader('Connection', 'keep-alive');

          try {
            const reader = response.body?.getReader();
            if (!reader) {
              throw new Error('Response body is not readable');
            }

            const decoder = new TextDecoder();
            let buffer = '';

            while (true) {
              const { done, value } = await reader.read();
              if (done) break;

              buffer += decoder.decode(value, { stream: true });
              const lines = buffer.split('\n');
              buffer = lines.pop() || ''; // Keep incomplete line in buffer

              for (const line of lines) {
                if (line.trim()) {
                  // Forward OpenAI SSE format directly
                  res.write(`${line}\n`);
                }
              }
            }

            res.end();
            console.log('OpenAI streaming response completed');
          } catch (streamError) {
            console.error('Error in OpenAI streaming:', streamError);
            // Fall back to mock streaming if OpenAI streaming fails
            res.setHeader('Content-Type', 'text/plain; charset=utf-8');
            res.setHeader('Cache-Control', 'no-cache');
            res.setHeader('Connection', 'keep-alive');

            const mockContent = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–µ. –ò—Å–ø–æ–ª—å–∑—É—é –æ–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç.\n\n–û—Ç–ª–∏—á–Ω–æ! –î–∞–≤–∞–π –∏–∑—É—á–∏–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –≤–º–µ—Å—Ç–µ!";

            const words = mockContent.split(' ');
            for (let i = 0; i < words.length; i++) {
              const chunk = {
                id: `chatcmpl-${Date.now()}`,
                object: 'chat.completion.chunk',
                created: Math.floor(Date.now() / 1000),
                model: selectedModel,
                choices: [{
                  index: 0,
                  delta: {
                    content: (i > 0 ? ' ' : '') + words[i]
                  },
                  finish_reason: null
                }]
              };

              res.write(`data: ${JSON.stringify(chunk)}\n\n`);
              await new Promise(resolve => setTimeout(resolve, 30));
            }

            const finalChunk = {
              id: `chatcmpl-${Date.now()}`,
              object: 'chat.completion.chunk',
              created: Math.floor(Date.now() / 1000),
              model: selectedModel,
              choices: [{
                index: 0,
                delta: {},
                finish_reason: 'stop'
              }]
            };

            res.write(`data: ${JSON.stringify(finalChunk)}\n\n`);
            res.write('data: [DONE]\n\n');
            res.end();
          }
          return;
        } else {
        const data = await response.json();
          console.log(`‚úÖ OpenAI API response received (model: ${selectedModel})`);
        return res.json(data);
        }
      } catch (error) {
        console.error('Error calling OpenAI API:', error);
        // Fall through to mock response
      }
    }

    // Generate mock response based on conversation context
    let mockContent = '';
    
    // Check conversation context for better responses
    const userMessages = messages.filter(m => m.role === 'user');
    const assistantMessages = messages.filter(m => m.role === 'assistant');

    if (assistantMessages.length === 0 && systemMessage) {
      // First message with system prompt - likely a lesson or general introduction
      const systemContent = systemMessage.content;
      
      if (systemContent.includes('—É—Ä–æ–∫') || systemContent.includes('–£—Ä–æ–∫') || systemContent.includes('–ü–†–ò –ü–ï–†–í–û–ú –°–û–û–ë–©–ï–ù–ò–ò')) {
        // Lesson welcome
        const courseTitleMatch = systemContent.match(/–ø–æ\s+["']?([^"']+)["']?/);
        const courseTitle = courseTitleMatch ? courseTitleMatch[1] : '–∞–Ω–≥–ª–∏–π—Å–∫–æ–º—É —è–∑—ã–∫—É';
        
        const lessonNumberMatch = systemContent.match(/—É—Ä–æ–∫\s+(\d+)/);
        const lessonNumber = lessonNumberMatch ? lessonNumberMatch[1] : '';
        
        mockContent = `–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –Ω–∞ —É—Ä–æ–∫ –ø–æ ${courseTitle}!${lessonNumber ? ` –≠—Ç–æ —É—Ä–æ–∫ –Ω–æ–º–µ—Ä ${lessonNumber}.` : ''}

–Ø –Æ–ª–∏—è, —Ç–≤–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π —É—á–∏—Ç–µ–ª—å. –°–µ–≥–æ–¥–Ω—è –º—ã –±—É–¥–µ–º –∏–∑—É—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ç–µ–º—ã –∏ —Ä–∞–∑–±–∏—Ä–∞—Ç—å –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —É —Ç–µ–±—è –≤–æ–∑–Ω–∏–∫–Ω—É—Ç.

–° —á–µ–≥–æ –±—ã —Ç—ã —Ö–æ—Ç–µ–ª –Ω–∞—á–∞—Ç—å? –ú–æ–∂–µ—Ç –±—ã—Ç—å, —É —Ç–µ–±—è –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–µ–∫—É—â–µ–π —Ç–µ–º–µ –∏–ª–∏ —Ö–æ—á–µ—à—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å –¥–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ?`;
      } else {
        mockContent = `–ü—Ä–∏–≤–µ—Ç! –Ø –Æ–ª–∏—è, —Ç–≤–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π —É—á–∏—Ç–µ–ª—å. 

${systemContent.includes('–ø—Ä–µ–¥–º–µ—Ç') ? '–Ø –ø–æ–º–æ–≥—É —Ç–µ–±–µ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è —Å –ª—é–±—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –ø–æ —É—á–µ–±–µ.' : '–ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å —Ç–µ–±–µ —Å–µ–≥–æ–¥–Ω—è?'}

–†–∞—Å—Å–∫–∞–∂–∏, —Å –∫–∞–∫–∏–º –ø—Ä–µ–¥–º–µ—Ç–æ–º –∏–ª–∏ —Ç–µ–º–æ–π —Ç–µ–±–µ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å?`;
      }
    } else if (lastUserMessage) {
      // Ongoing conversation - respond based on user input
      const userInput = lastUserMessage.content?.toLowerCase() || '';

      if (userInput.includes('–∞–Ω–≥–ª–∏–π—Å–∫') || userInput.includes('english') || userInput.includes('–∏–∑—É—á–∏—Ç—å')) {
        mockContent = `–û—Ç–ª–∏—á–Ω–æ! –î–∞–≤–∞–π –∏–∑—É—á–∏–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –≤–º–µ—Å—Ç–µ! 

–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Ç–µ–±–µ —Å:
‚Ä¢ üìù –û—Å–Ω–æ–≤–∞–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ (–≤—Ä–µ–º–µ–Ω–∞, –∞—Ä—Ç–∏–∫–ª–∏, –ø—Ä–µ–¥–ª–æ–≥–∏)
‚Ä¢ üó£Ô∏è –†–∞–∑–≤–∏—Ç–∏–µ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤
‚Ä¢ üìñ –ò–∑—É—á–µ–Ω–∏–µ–º –Ω–æ–≤—ã—Ö —Å–ª–æ–≤ –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏–π
‚Ä¢ ‚úçÔ∏è –ü—Ä–∞–∫—Ç–∏–∫–æ–π –ø–∏—Å—å–º–∞
‚Ä¢ üëÇ –†–∞–∑–≤–∏—Ç–∏–µ–º –Ω–∞–≤—ã–∫–æ–≤ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—á–∏

–ß—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ —Ç—ã —Ö–æ—á–µ—à—å –∏–∑—É—á–∏—Ç—å? –ú–æ–∂–µ—Ç –±—ã—Ç—å, –Ω–∞—á–Ω–µ–º —Å –æ—Å–Ω–æ–≤ –∏–ª–∏ —É —Ç–µ–±—è –µ—Å—Ç—å –∫–∞–∫–∏–µ-—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç–µ–±—è –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç?`;
      } else if (userInput.includes('–¥–æ–º–∞—à–Ω') || userInput.includes('–∑–∞–¥–∞–Ω–∏') || userInput.includes('homework')) {
        mockContent = `–ö–æ–Ω–µ—á–Ω–æ, –ø–æ–º–æ–≥—É —Å –¥–æ–º–∞—à–Ω–∏–º –∑–∞–¥–∞–Ω–∏–µ–º! –†–∞—Å—Å–∫–∞–∂–∏, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å:

‚Ä¢ –ö–∞–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ?
‚Ä¢ –ü–æ –∫–∞–∫–æ–π —Ç–µ–º–µ?
‚Ä¢ –ï—Å—Ç—å –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏?

–Ø –æ–±—ä—è—Å–Ω—é –º–∞—Ç–µ—Ä–∏–∞–ª –∏ –ø–æ–º–æ–≥—É —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è —Å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è–º–∏.`;
      } else if (userInput.includes('—Ç–µ—Å—Ç') || userInput.includes('—ç–∫–∑–∞–º–µ–Ω') || userInput.includes('–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω')) {
        mockContent = `–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Ç–µ—Å—Ç–∞–º - —ç—Ç–æ –≤–∞–∂–Ω–æ! –Ø –ø–æ–º–æ–≥—É —Ç–µ–±–µ:

‚Ä¢ –ü–æ–≤—Ç–æ—Ä–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
‚Ä¢ –†–∞–∑–æ–±—Ä–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
‚Ä¢ –ü–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ —Ä–µ—à–µ–Ω–∏–∏ —Ç–∏–ø–∏—á–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π
‚Ä¢ –î–∞—Ç—å —Å–æ–≤–µ—Ç—ã –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ

–ü–æ –∫–∞–∫–æ–º—É –ø—Ä–µ–¥–º–µ—Ç—É –≥–æ—Ç–æ–≤–∏—à—å—Å—è –∏ –∫–∞–∫–∏–µ —Ç–µ–º—ã –Ω—É–∂–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å?`;
      } else {
        mockContent = `–•–æ—Ä–æ—à–æ, –¥–∞–≤–∞–π —Ä–∞–∑–±–µ—Ä–µ–º —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å! 

${lastUserMessage.content}

–Ø –ø–æ–º–æ–≥—É —Ç–µ–±–µ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è —Å —ç—Ç–æ–π —Ç–µ–º–æ–π. –†–∞—Å—Å–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ, —á—Ç–æ –∏–º–µ–Ω–Ω–æ —Ç–µ–±–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ –∏–ª–∏ —á—Ç–æ —Ç—ã —Ö–æ—á–µ—à—å —É–∑–Ω–∞—Ç—å?`;
      }
    } else {
      mockContent = `–ü—Ä–∏–≤–µ—Ç! –Ø –Æ–ª–∏—è, —Ç–≤–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π —É—á–∏—Ç–µ–ª—å –ø–æ –≤—Å–µ–º —à–∫–æ–ª—å–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º.

–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Ç–µ–±–µ —Å:
‚Ä¢ üìö –û–±—ä—è—Å–Ω–µ–Ω–∏–µ–º —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–º
‚Ä¢ ‚úèÔ∏è –†–µ—à–µ–Ω–∏–µ–º –¥–æ–º–∞—à–Ω–∏—Ö –∑–∞–¥–∞–Ω–∏–π
‚Ä¢ üéØ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–æ–π –∫ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–º –∏ —ç–∫–∑–∞–º–µ–Ω–∞–º
‚Ä¢ ‚ùì –û—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –ª—é–±—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —É—á–µ–±–µ

–†–∞—Å—Å–∫–∞–∂–∏, —Å –∫–∞–∫–∏–º –ø—Ä–µ–¥–º–µ—Ç–æ–º –∏–ª–∏ —Ç–µ–º–æ–π —Ç–µ–±–µ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å?`;
    }

    if (stream) {
      // Send streaming response in SSE format
      console.log('Sending mock streaming chat completion response');
      res.setHeader('Content-Type', 'text/plain; charset=utf-8');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');

      const words = mockContent.split(' ');
      let currentText = '';

      for (let i = 0; i < words.length; i++) {
        currentText += (i > 0 ? ' ' : '') + words[i];

        const chunk = {
          id: `chatcmpl-${Date.now()}`,
          object: 'chat.completion.chunk',
          created: Math.floor(Date.now() / 1000),
          model: selectedModel,
          choices: [{
            index: 0,
            delta: {
              content: (i > 0 ? ' ' : '') + words[i]
            },
            finish_reason: null
          }]
        };

        res.write(`data: ${JSON.stringify(chunk)}\n\n`);

        // Small delay to simulate streaming
        await new Promise(resolve => setTimeout(resolve, 50));
      }

      // Send completion chunk
      const finalChunk = {
        id: `chatcmpl-${Date.now()}`,
        object: 'chat.completion.chunk',
        created: Math.floor(Date.now() / 1000),
        model: selectedModel,
        choices: [{
          index: 0,
          delta: {},
          finish_reason: 'stop'
        }]
      };

      res.write(`data: ${JSON.stringify(finalChunk)}\n\n`);
      res.write('data: [DONE]\n\n');
      res.end();
    } else {
      // Return regular JSON response
    const mockResponse = {
      id: `chatcmpl-${Date.now()}`,
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
        model: selectedModel,
      choices: [
        {
          index: 0,
          message: {
            role: 'assistant',
            content: mockContent
          },
          finish_reason: 'stop'
        }
      ],
      usage: {
        prompt_tokens: 100,
          completion_tokens: mockContent.length / 4,
          total_tokens: 100 + mockContent.length / 4
      }
    };

    console.log('Sending mock chat completion response');
    res.json(mockResponse);
    }
  } catch (error) {
    console.error('Error in chat completions:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

/**
 * @route   POST /api/audio/speech
 * @desc    Generate speech from text (OpenAI TTS)
 * @access  Private
 */
router.post('/audio/speech', async (req, res) => {
  try {
    console.log('TTS request received:', {
      model: req.body.model,
      inputLength: req.body.input?.length,
      voice: req.body.voice,
      response_format: req.body.response_format,
      speed: req.body.speed
    });

    const {
      model,
      input,
      voice = 'alloy',
      response_format = 'mp3',
      speed = 1.0
    } = req.body;

    if (!input) {
      console.log('Missing input parameter in TTS request');
      return res.status(400).json({ error: 'Missing input parameter' });
    }

    // Check if OpenAI API key is configured
    const openaiApiKey = process.env.OPENAI_API_KEY;
    if (!openaiApiKey) {
      console.log('OpenAI API key not configured, returning mock response');
      // Return mock audio response as fallback
      const sampleRate = 22050;
      const channels = 1;
      const bitsPerSample = 16;
      const duration = 3;
      const dataSize = sampleRate * channels * bitsPerSample / 8 * duration;

      const header = Buffer.alloc(44);
      header.write('RIFF', 0);
      header.writeUInt32LE(36 + dataSize, 4);
      header.write('WAVE', 8);
      header.write('fmt ', 12);
      header.writeUInt32LE(16, 16);
      header.writeUInt16LE(1, 20);
      header.writeUInt16LE(channels, 22);
      header.writeUInt32LE(sampleRate, 24);
      header.writeUInt32LE(sampleRate * channels * bitsPerSample / 8, 28);
      header.writeUInt16LE(channels * bitsPerSample / 8, 32);
      header.writeUInt16LE(bitsPerSample, 34);
      header.write('data', 36);
      header.writeUInt32LE(dataSize, 40);

      const audioData = Buffer.alloc(dataSize, 0);
      const mockAudioData = Buffer.concat([header, audioData]);

      res.setHeader('Content-Type', 'audio/wav');
      res.setHeader('Content-Length', mockAudioData.length);
      return res.send(mockAudioData);
    }

    // Use OpenAI TTS API
    console.log(`üé§ Calling OpenAI TTS API with model: ${model || 'tts-1'}, voice: ${voice}, format: ${response_format}`);

    const openaiResponse = await fetch('https://api.openai.com/v1/audio/speech', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiApiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: model || 'tts-1',
        input: input,
        voice: voice,
        response_format: response_format,
        speed: speed,
      }),
    });

    if (!openaiResponse.ok) {
      const errorData = await openaiResponse.text();
      console.error('OpenAI TTS API error:', errorData);
      throw new Error(`OpenAI TTS API error: ${openaiResponse.status}`);
    }

    // Get the content type from OpenAI response
    const contentType = openaiResponse.headers.get('content-type') || 'audio/mpeg';

    // Stream the audio response directly to client
    res.setHeader('Content-Type', contentType);
    res.setHeader('Cache-Control', 'no-cache');

    // –ü–æ–ª—É—á–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é (–ù–ï –∫–∞–∫ —Ç–µ–∫—Å—Ç!)
    const audioBuffer = await openaiResponse.arrayBuffer();
    const audioData = Buffer.from(audioBuffer);
    
    console.log('‚úÖ OpenAI TTS audio received, size:', audioData.length, 'bytes');
    
    res.setHeader('Content-Length', audioData.length);
    res.send(audioData);
    
    console.log('‚úÖ OpenAI TTS audio response sent to client');

  } catch (error) {
    console.error('Error in OpenAI TTS:', error);

    // Return mock audio as fallback
    const sampleRate = 22050;
    const channels = 1;
    const bitsPerSample = 16;
    const duration = 3;
    const dataSize = sampleRate * channels * bitsPerSample / 8 * duration;

    const header = Buffer.alloc(44);
    header.write('RIFF', 0);
    header.writeUInt32LE(36 + dataSize, 4);
    header.write('WAVE', 8);
    header.write('fmt ', 12);
    header.writeUInt32LE(16, 16);
    header.writeUInt16LE(1, 20);
    header.writeUInt16LE(channels, 22);
    header.writeUInt32LE(sampleRate, 24);
    header.writeUInt32LE(sampleRate * channels * bitsPerSample / 8, 28);
    header.writeUInt16LE(channels * bitsPerSample / 8, 32);
    header.writeUInt16LE(bitsPerSample, 34);
    header.write('data', 36);
    header.writeUInt32LE(dataSize, 40);

    const audioData = Buffer.alloc(dataSize, 0);
    const mockAudioData = Buffer.concat([header, audioData]);

    res.setHeader('Content-Type', 'audio/wav');
    res.setHeader('Content-Length', mockAudioData.length);
    res.send(mockAudioData);
  }
});

module.exports = router;
