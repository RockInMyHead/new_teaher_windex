import React, { useState, useRef, useEffect } from 'react';
import { useNavigate, useSearchParams } from 'react-router-dom';
import { Button } from '@/components/ui/button';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { ArrowLeft, Mic, Loader2, MicOff, PhoneOff } from 'lucide-react';
import { HeaderWithHero } from '@/components/Header';
import { useLearningProfile } from '@/hooks/useLearningProfile';
import { sessionService } from '@/services/sessionService';
import { learningProfileService } from '@/services/learningProfileService';
import { parseCourseId, getFullCourseTitle, getCourseById } from '@/config/courses';
import { OpenAITTS } from '@/lib/openaiTTS';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
}

const VoiceCallPage: React.FC = () => {
  const navigate = useNavigate();
  const [searchParams] = useSearchParams();
  
  // Get course ID from URL params
  const courseIdFromParams = searchParams.get('course') || '';
  const userIdFromStorage = sessionService.getUserId();

  // Learning profile hook - loads student profile and LLM context
  const {
    profile: learningProfile,
    llmContext,
    isLoading: isLoadingProfile,
    systemPrompt: profileSystemPrompt,
    analyzeAndUpdateFromLLM,
    loadLLMContext
  } = useLearningProfile({
    userId: userIdFromStorage,
    courseId: courseIdFromParams,
    autoLoad: !!courseIdFromParams
  });

  // State
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [messages, setMessages] = useState<Message[]>([]);
  const [error, setError] = useState<string | null>(null);
  const [speechTheses, setSpeechTheses] = useState<string[]>([]);
  const [audioBlocked, setAudioBlocked] = useState(false);

  // Lesson tracking
  const lessonStartTimeRef = useRef<Date | null>(null);
  
  // Use ref for lesson context to avoid closure issues
  const lessonContextRef = useRef<{
    title: string;
    topic: string;
    description: string;
  } | null>(null);

  // Refs
  const audioStreamRef = useRef<MediaStream | null>(null);
  const isActiveRef = useRef<boolean>(false);
  // videoRef removed - using CSS animated avatar instead
  // Web Speech Recognition instance
  const recognitionRef = useRef<any>(null);

  // Audio analysis refs
  const analyserRef = useRef<any>(null);
  const animationFrameRef = useRef<number | null>(null);

  // Audio detection refs
  const speechFramesRef = useRef<number>(0);
  const silenceFramesRef = useRef<number>(0);
  const silenceAfterSpeechRef = useRef<number>(0);
  const speechDetectedRef = useRef<boolean>(false);
  const processingTypeRef = useRef<string | null>(null);

  // Audio calibration refs
  const isCalibrationDoneRef = useRef<boolean>(false);
  const calibrationSamplesRef = useRef<number[]>([]);
  const noiseFloorRef = useRef<number>(0);
  const isQuickCalibrationRef = useRef<boolean>(false);

  // Media recording refs
  const mediaRecorderRef = useRef<any>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  
  // TTS Audio ref for cleanup
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  
  // Profile creation tracking
  const profileCreationAttemptedRef = useRef<boolean>(false);
  
  // Initialization tracking
  const initializationStartedRef = useRef<boolean>(false);

  // Audio detection constants
  const MIN_THRESHOLD = 5;
  const REQUIRED_SPEECH_FRAMES = 30;
  const MIN_SPEECH_DURATION = 15;
  const SILENCE_AFTER_SPEECH_FRAMES = 90;
  const QUICK_CALIBRATION_FRAMES = 30;
  const CALIBRATION_FRAMES = 150;

  // Web Speech API parameters
  const SILENCE_TIMEOUT = 2000; // 2 seconds of silence to consider speech ended

  // Toggle microphone mute/unmute
  const toggleMute = () => {
    if (isMuted) {
      // Unmute - resume listening
      setIsMuted(false);
      console.log('üé§ Microphone unmuted');
      if (!isListening && !isProcessing) {
        startListening();
      }
    } else {
      // Mute - stop listening
      setIsMuted(true);
      console.log('üîá Microphone muted');
      stopListening();
    }
    // Hide audio blocked indicator after user interaction
    if (audioBlocked) {
      setAudioBlocked(false);
    }
  };

  // End lesson and navigate back
  const endLesson = async () => {
    console.log('üìû Ending lesson');

    // Evaluate lesson if we have enough data
    if (lessonStartTimeRef.current && messages.length > 1 && userIdFromStorage && courseIdFromParams) {
      try {
        console.log('üìä Evaluating lesson...');
        const lessonTitle = lessonContextRef.current?.title || '–ì–æ–ª–æ—Å–æ–≤–æ–π —É—Ä–æ–∫';
        const lessonTopic = lessonContextRef.current?.topic || '';

        // Convert messages to conversation format
        const conversationHistory = messages.map(msg => ({
          role: msg.role === 'user' ? 'user' : 'assistant',
          content: msg.content
        }));

        // Evaluate and save lesson assessment
        await learningProfileService.evaluateLesson(
          userIdFromStorage,
          courseIdFromParams,
          lessonTitle,
          lessonTopic,
          conversationHistory,
          lessonStartTimeRef.current,
          new Date()
        );

        console.log('‚úÖ Lesson evaluation completed');
      } catch (error) {
        console.error('‚ùå Error evaluating lesson:', error);
      }
    }

    stopListening();
    cleanup();
    setSpeechTheses([]);
    // Hide audio blocked indicator after user interaction
    if (audioBlocked) {
      setAudioBlocked(false);
    }
    navigate(-1);
  };

  // Cleanup function
  // Stop TTS function (called when user starts speaking)
  const stopTTS = () => {
    console.log('üîá Interrupting TTS due to user speech...');

    // Stop OpenAI TTS streaming
    OpenAITTS.stop();

    // Stop HTML Audio TTS (legacy fallback)
    if (currentAudioRef.current) {
      console.log('üîá Stopping TTS audio...');
      currentAudioRef.current.pause();
      currentAudioRef.current.src = '';
      currentAudioRef.current = null;
    }

    // Stop browser TTS (Speech Synthesis)
    if ('speechSynthesis' in window && window.speechSynthesis.speaking) {
      console.log('üîá Stopping Speech Synthesis...');
      window.speechSynthesis.cancel();
    }
  };

  const cleanup = () => {
    console.log('üßπ Cleanup started');
    
    // Stop OpenAI TTS streaming
    OpenAITTS.stop();
    
    // Stop TTS Audio (legacy fallback)
    if (currentAudioRef.current) {
      console.log('üîá Stopping TTS audio...');
      currentAudioRef.current.pause();
      currentAudioRef.current.src = '';
      currentAudioRef.current = null;
    }
    
    // Stop browser TTS (Speech Synthesis)
    if ('speechSynthesis' in window && window.speechSynthesis.speaking) {
      console.log('üîá Stopping Speech Synthesis...');
      window.speechSynthesis.cancel();
    }
    
    // Stop Web Speech Recognition
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
      } catch (e) {
        console.warn('Speech recognition stop error:', e);
      }
      recognitionRef.current = null;
    }
    
    // Stop audio stream
    if (audioStreamRef.current) {
      audioStreamRef.current.getTracks().forEach(track => track.stop());
      audioStreamRef.current = null;
    }
    
    isActiveRef.current = false;
    
    console.log('‚úÖ Cleanup complete');
  };

  // Stop audio recording
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
      console.log('üõë Recording stopped');
    }
  };

  // Setup audio analysis for speech detection
  const setupAudioAnalysis = (stream: MediaStream) => {
    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(stream);

      analyser.fftSize = 256;
      analyser.smoothingTimeConstant = 0.8;
      analyser.minDecibels = -90;
      analyser.maxDecibels = -10;

      source.connect(analyser);
      analyserRef.current = analyser;

      console.log('üéµ Audio analysis setup complete');
    } catch (error) {
      console.error('‚ùå Audio analysis setup failed:', error);
    }
  };

  // Handle speech audio processing
  const handleSpeech = async (audioBlob: Blob) => {
    try {
      console.log('üé§ Processing speech audio...', audioBlob.size, 'bytes');

      setIsProcessing(true);
      setError(null);

      // Convert blob to base64 for API
      const reader = new FileReader();
      reader.onloadend = async () => {
        try {
          const base64Audio = reader.result as string;

          // Send to Whisper API for transcription
          const response = await fetch('/api/transcribe', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              audio: base64Audio,
              language: 'ru'
            })
          });

          if (!response.ok) {
            throw new Error(`Transcription failed: ${response.statusText}`);
          }

          const result = await response.json();
          const transcript = result.transcript || '';

          if (transcript.trim()) {
            console.log('üìù Transcribed:', transcript);
            await handleSpeechTranscript(transcript.trim());
          } else {
            console.log('ü§∑ Empty transcript, resuming listening...');
            resumeListening();
          }
        } catch (error) {
          console.error('‚ùå Transcription error:', error);
          setError('–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏');
          resumeListening();
        } finally {
          setIsProcessing(false);
        }
      };

      reader.readAsDataURL(audioBlob);
    } catch (error) {
      console.error('‚ùå Speech handling error:', error);
      setIsProcessing(false);
      resumeListening();
    }
  };

  // Start Web Speech API listening
  const startListening = async () => {
    if (isActiveRef.current) {
      console.log('‚ö†Ô∏è Already active, skipping start');
      return;
    }

    try {
      console.log('üé§ Starting Web Speech API listening...');

      // Check if Web Speech API is available
      const SpeechRecognition = window.SpeechRecognition || (window as any).webkitSpeechRecognition;

      if (!SpeechRecognition) {
        throw new Error('Web Speech API not supported in this browser');
      }

      // Cleanup any existing recognition
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }

      // Get microphone access (required for Speech Recognition)
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      audioStreamRef.current = stream;
      console.log('‚úÖ Microphone access granted');

      // Create new recognition instance
      const recognition = new SpeechRecognition();
      recognitionRef.current = recognition;

      // Configure recognition
      recognition.continuous = true; // Keep listening until stopped
      recognition.interimResults = true; // Get intermediate results
      recognition.lang = 'ru-RU'; // Russian language
      recognition.maxAlternatives = 1;

      isActiveRef.current = true;
      setIsListening(true);
      setError(null);

      let finalTranscript = '';
      let interimTranscript = '';

      recognition.onstart = () => {
        console.log('üéôÔ∏è Web Speech Recognition started');
        // TTS will be stopped automatically when new speech starts
      };

      recognition.onresult = async (event) => {
        interimTranscript = '';

        // Process all results
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;

          if (event.results[i].isFinal) {
            finalTranscript += transcript;
            console.log('üé§ Final result:', transcript);

            // Process the final transcript
            if (transcript.trim().length > 0) {
              await handleSpeechTranscript(transcript.trim());
            }
          } else {
            interimTranscript += transcript;
            // Removed frequent interim result logging for performance
          }
        }
      };

      recognition.onerror = (event) => {
        console.error('‚ùå Speech recognition error:', event.error);

        if (event.error === 'not-allowed') {
          setError('–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â–µ–Ω. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.');
          setIsListening(false);
        } else if (event.error === 'no-speech') {
          console.log('ü§´ No speech detected, continuing...');
          // Continue listening - this is normal
        } else if (event.error === 'network') {
          console.warn('üåê Network error, will retry...');
          setError('–ü—Ä–æ–±–ª–µ–º–∞ —Å —Å–µ—Ç—å—é, –ø—ã—Ç–∞–µ–º—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å...');
          // Try to restart after a delay
          setTimeout(() => {
            if (isActiveRef.current) {
              console.log('üîÑ Retrying speech recognition after network error...');
              startListening();
            }
          }, 2000);
        } else if (event.error === 'audio-capture') {
          console.warn('üé§ Audio capture error, restarting...');
          setError('–ü—Ä–æ–±–ª–µ–º–∞ —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–æ–º, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º...');
          // Try to restart listening
          setTimeout(() => {
            if (isActiveRef.current) {
              console.log('üîÑ Retrying speech recognition after audio capture error...');
              startListening();
            }
          }, 1000);
        } else if (event.error === 'not-available') {
          setError('–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ –≤ —ç—Ç–æ–º –±—Ä–∞—É–∑–µ—Ä–µ');
          setIsListening(false);
        } else {
          console.error('‚ùå Unhandled speech recognition error:', event.error);
          setError(`–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏: ${event.error}`);
          setIsListening(false);
        }
      };

      recognition.onend = () => {
        console.log('üé§ Speech recognition ended');

        // Restart if still active (unless it was stopped intentionally)
        if (isActiveRef.current) {
          console.log('üîÑ Auto-restarting speech recognition...');
          setTimeout(() => startListening(), 100);
        }
      };

      // Start recognition
      recognition.start();
      console.log('üé§ Web Speech Recognition initiated');

    } catch (error) {
      console.error('‚ùå Start listening error:', error);

      if (error instanceof Error && error.message.includes('Web Speech API not supported')) {
        setError('–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏');
      } else {
      setError('–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É');
      }

      isActiveRef.current = false;
      setIsListening(false);
    }
  };


  // Detect audio levels with adaptive noise floor
  const detectAudio = () => {
    if (!isActiveRef.current || !analyserRef.current) {
      console.log('üõë Detection stopped');
      return;
    }

    const analyser = analyserRef.current;
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    analyser.getByteFrequencyData(dataArray);
    
    // Calculate average and max energy in voice frequency range (roughly bins 10-100 for typical sample rates)
    // Human voice is typically 85-255 Hz (low) to 3400 Hz (high)
    // We focus on bins that represent ~300-3000 Hz
    const voiceStartBin = Math.floor(bufferLength * 0.05); // ~5% of spectrum
    const voiceEndBin = Math.floor(bufferLength * 0.4); // ~40% of spectrum
    
    let sum = 0;
    let max = 0;
    let count = 0;
    
    for (let i = voiceStartBin; i < voiceEndBin && i < bufferLength; i++) {
      sum += dataArray[i];
      if (dataArray[i] > max) max = dataArray[i];
      count++;
    }
    
    const average = count > 0 ? sum / count : 0;
    
    // Calibration phase: measure background noise
    if (!isCalibrationDoneRef.current) {
      // Only add reasonable samples to calibration (filter out extreme spikes)
      if (average > 1 && average < 80) {
        calibrationSamplesRef.current.push(average);
      }
      
      // Use quick calibration (0.5s) for resume, full calibration (1.5s) for initial start
      const requiredFrames = isQuickCalibrationRef.current ? QUICK_CALIBRATION_FRAMES : CALIBRATION_FRAMES;
      
      if (calibrationSamplesRef.current.length >= requiredFrames) {
        // Calculate noise floor as average of calibration samples
        const noiseSum = calibrationSamplesRef.current.reduce((a, b) => a + b, 0);
        const measuredNoiseFloor = noiseSum / calibrationSamplesRef.current.length;
        
        // Set minimum noise floor to avoid zero threshold (optimized for quiet environments)
        noiseFloorRef.current = Math.max(measuredNoiseFloor, 3);
        
        isCalibrationDoneRef.current = true;
        const calibType = isQuickCalibrationRef.current ? 'Quick' : 'Full';
        console.log(`üéöÔ∏è ${calibType} calibration: measured=${measuredNoiseFloor.toFixed(2)}, actual=${noiseFloorRef.current.toFixed(2)}, threshold=${(noiseFloorRef.current * 2.0).toFixed(2)}`);
      } else {
        // Still calibrating, continue
        animationFrameRef.current = requestAnimationFrame(detectAudio);
        return;
      }
    }
    
    // Dynamic speech threshold: noise floor * 1.5 (adaptive to environment)
    const MIN_THRESHOLD = 8; // –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø–æ—Ä–æ–≥ (lower for quiet speech)
    const dynamicThreshold = Math.max(noiseFloorRef.current * 1.5, MIN_THRESHOLD);

    // Periodic logging to debug detection issues (every 50 frames = ~2.5 seconds)
    if (speechFramesRef.current === 0 && silenceFramesRef.current % 50 === 0 && silenceFramesRef.current > 0) {
      console.log(`üëÇ Listening... avg=${average.toFixed(1)}, max=${max}, threshold=${dynamicThreshold.toFixed(1)} (normal speaking volume)`);
    }

    // –ü–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–∏—à–∏–Ω—ã
    const silenceThreshold = speechDetectedRef.current
      ? Math.max(dynamicThreshold * 0.5, MIN_THRESHOLD * 0.4) // –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –ø–æ—Å–ª–µ —Ä–µ—á–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
      : dynamicThreshold; // –ü–æ—Ä–æ–≥ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏

    // –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ä–µ—á–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–æ–π –ø–æ—Ä–æ–≥–æ–≤
    let effectiveSilenceThreshold = silenceThreshold;

    // –ï—Å–ª–∏ —É—Ä–æ–≤–Ω–∏ —Å–∏–≥–Ω–∞–ª–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–æ–≤, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥–∏
    // –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —Å —Ç–∏—Ö–æ–π —Ä–µ—á—å—é –≤ —Ç–∏—Ö–æ–π —Å—Ä–µ–¥–µ
    if (speechDetectedRef.current && silenceFramesRef.current > 100) {
      // –ü–æ—Å–ª–µ 5 —Å–µ–∫—É–Ω–¥ —Ç–∏—à–∏–Ω—ã –ø—Ä–∏ –∞–∫—Ç–∏–≤–Ω–æ–π —Ä–µ—á–∏ - —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥
      effectiveSilenceThreshold = Math.max(silenceThreshold * 0.6, MIN_THRESHOLD * 0.3);
      console.log(`üéöÔ∏è Auto-adjusted silence threshold: ${effectiveSilenceThreshold.toFixed(1)} (was ${silenceThreshold.toFixed(1)})`);
    }

    const isSpeech = speechDetectedRef.current
      ? average > effectiveSilenceThreshold || max > noiseFloorRef.current * 1.8 // –ü–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏ - –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ
      : average > dynamicThreshold || max > noiseFloorRef.current * 2.2; // –î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏
    
    if (isSpeech) {
      // Speech detected
      speechFramesRef.current++;
      silenceAfterSpeechRef.current = 0;

      // Mark that speech was detected
      if (speechFramesRef.current >= REQUIRED_SPEECH_FRAMES && !speechDetectedRef.current) {
        // Removed detailed speech analysis logging for performance
        speechDetectedRef.current = true;
      }
      
      // Log every 100 frames to monitor (less verbose)
      if (speechDetectedRef.current && speechFramesRef.current % 100 === 0) {
        console.log(`üó£Ô∏è Speaking... frames=${speechFramesRef.current}, avg=${average.toFixed(1)}, max=${max}, silence_threshold=${silenceThreshold.toFixed(1)}`);
      }
    } else {
      // Silence detected
      if (speechDetectedRef.current) {
        // We detected speech earlier, now counting silence after it
        silenceAfterSpeechRef.current++;
        
        if (silenceAfterSpeechRef.current === 1) {
          console.log(`ü§´ Silence detected: avg=${average.toFixed(1)}, silence_threshold=${silenceThreshold.toFixed(1)}`);
        }
        
        if (silenceAfterSpeechRef.current % 30 === 0 && silenceAfterSpeechRef.current > 1) {
          console.log(`ü§´ Silence progress: ${silenceAfterSpeechRef.current}/${SILENCE_AFTER_SPEECH_FRAMES}, avg=${average.toFixed(1)}`);
        }
        
        if (silenceAfterSpeechRef.current >= SILENCE_AFTER_SPEECH_FRAMES) {
          // Check minimum speech duration (at least 8 frames = ~0.4 seconds)
          const MIN_SPEECH_DURATION = 8;
          if (speechFramesRef.current >= MIN_SPEECH_DURATION) {
            console.log(`‚úÖ SPEECH ENDED after ${silenceAfterSpeechRef.current} frames of silence (${speechFramesRef.current} speech frames)`);
          processingTypeRef.current = 'speech';
          stopRecording();
          } else {
            console.log(`‚ö†Ô∏è Speech too short (${speechFramesRef.current} frames), restarting listening...`);
            restartListening();
          }
          return;
        }
      } else {
        // No speech yet, just reset speech counter and continue listening
        silenceFramesRef.current++;
        speechFramesRef.current = 0;
        
        // Don't generate follow-up questions on silence - just keep listening
        // User will speak when ready
      }
    }

    // Continue detection
    animationFrameRef.current = requestAnimationFrame(detectAudio);
  };

  // Stop listening
  const stopListening = () => {
    console.log('‚èπÔ∏è Stop listening called');

    // Stop Web Speech Recognition
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
      } catch (e) {
        console.warn('Speech recognition stop error:', e);
      }
      recognitionRef.current = null;
    }

    isActiveRef.current = false;
    setIsListening(false);
    console.log('‚úÖ Stop listening complete');
  };

  // Handle speech transcript from Web Speech API
  const handleSpeechTranscript = async (transcript: string) => {
    // Prevent concurrent processing
    if (isProcessing) {
      console.warn('‚ö†Ô∏è Already processing speech, skipping...');
      return;
    }

    try {
      console.log('üîä Processing speech transcript...');
      setIsProcessing(true);

      // Use transcript directly from Web Speech API
      const transcription = transcript;

      if (!transcription || transcription.trim().length < 2) {
        console.warn('‚ö†Ô∏è Transcription too short');
        setIsProcessing(false);
        return;
      }

      // Basic validation - Web Speech API is usually reliable
      const hasOnlyEmoji = /^[\p{Emoji}\s]+$/u.test(transcription.trim());
      const hasWeirdChars = /[^\w\s–∞-—è—ë\-.,!?;:()"¬´¬ª‚Äî‚Äì‚Ä¶‚Ññ√∑√ó¬±=‚â†<>‚â§‚â•‚àö‚àõ‚àú‚à´‚àë‚àè‚àÜ‚àû‚àû¬∞%‚Ä∞‚Ä±\s]/gi.test(transcription.trim());

      if (hasOnlyEmoji || hasWeirdChars) {
        console.warn('‚ö†Ô∏è Transcription contains only emoji or weird characters:', transcription);
        setIsProcessing(false);
        return;
      }
      
      // Add user message
      setMessages(prev => [...prev, {
        role: 'user',
        content: transcription,
        timestamp: new Date()
      }]);

      // Get LLM response
      console.log('üì§ Getting LLM response for transcription:', transcription.substring(0, 100) + '...');
      let response = await getLLMResponse(transcription);
      console.log('ü§ñ LLM response received, length:', response ? response.length : 0);
      
      // Handle empty or too short responses with fallback
      if (!response || response.trim().length < 10) {
        console.warn('‚ö†Ô∏è Empty LLM response, using fallback message');
        response = `–•–æ—Ä–æ—à–æ, —Ç—ã —Å–∫–∞–∑–∞–ª: "${transcription}". –î–∞–≤–∞–π —Ä–∞–∑–±–µ—Ä—ë–º —ç—Ç–æ –ø–æ–¥—Ä–æ–±–Ω–µ–µ. –ß—Ç–æ –∏–º–µ–Ω–Ω–æ —Ç–µ–±–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ?`;
      }
      
      // Extract theses from response
      const theses = extractTheses(response);
      setSpeechTheses(theses);
      
      // Clean response from headers for TTS and display
      const cleanResponse = cleanMarkdownHeaders(response);
      
      // Use cleaned response for TTS and display
      let textForTTS = cleanResponse;
      
      // Add assistant message (using cleaned response)
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: cleanResponse,
        timestamp: new Date()
      }]);

      setIsProcessing(false);

      // Speak response (without theses)
      setIsSpeaking(true);
      await speakText(textForTTS);
      setIsSpeaking(false);

      // Resume listening after TTS with delay to prevent audio conflicts
      setTimeout(() => {
        if (isActiveRef.current) {
          startListening();
        }
      }, 500);
      
    } catch (error) {
      console.error('‚ùå Handle speech transcript error:', error);
      setIsProcessing(false);
      setIsSpeaking(false);
      // Try to restart listening
      setTimeout(() => {
        if (isActiveRef.current) {
          startListening();
        }
      }, 1000);
    }
  };

  // Send welcome message when entering chat
  const sendWelcomeMessage = async () => {
    try {
      console.log('üëã Sending welcome message...');
      setIsProcessing(true);

      // Get welcome message from LLM
      const welcomeMessage = await getLLMResponse('');

      // Extract theses from response
      const theses = extractTheses(welcomeMessage);
      setSpeechTheses(theses);

      // Clean response from headers for TTS and display
      const cleanResponse = cleanMarkdownHeaders(welcomeMessage);

      // Add assistant message
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: cleanResponse,
        timestamp: new Date()
      }]);

      setIsProcessing(false);
      setIsSpeaking(true);
      await speakText(cleanResponse);
      setIsSpeaking(false);

      console.log('‚úÖ Welcome message sent');
    } catch (error) {
      console.error('‚ùå Error sending welcome message:', error);
      setIsProcessing(false);
    }
  };

  // Handle silence
  const handleSilence = async () => {
    try {
      console.log('ü§´ Processing silence...');
      setIsProcessing(true);

      const message = "–ï—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã? –Ø –≥–æ—Ç–æ–≤–∞ –ø–æ–º–æ—á—å!";
      
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: message,
        timestamp: new Date()
      }]);

      setIsProcessing(false);
      setIsSpeaking(true);
      await speakText(message);
      setIsSpeaking(false);

      // Add delay before restarting to prevent echo
      console.log('‚è∏Ô∏è Waiting 2 seconds before restart to prevent echo...');
      await new Promise(resolve => setTimeout(resolve, 2000));

      restartListening();

    } catch (error) {
      console.error('‚ùå Handle silence error:', error);
      setIsProcessing(false);
      setIsSpeaking(false);
      restartListening();
    }
  };

  // Resume listening after TTS with delay to prevent audio conflicts
  const resumeListening = async () => {
    if (isActiveRef.current) {
      console.log('‚ö†Ô∏è Already active, skipping resume');
      return;
    }

    try {
      console.log('‚ö° Resuming listening after TTS...');
      
      // Reset detection state
      speechFramesRef.current = 0;
      silenceFramesRef.current = 0;
      silenceAfterSpeechRef.current = 0;
      speechDetectedRef.current = false;
      processingTypeRef.current = null;
      
      // Quick recalibration (0.5s) to adapt to current noise level
      isCalibrationDoneRef.current = false;
      calibrationSamplesRef.current = [];
      isQuickCalibrationRef.current = true;
      
      isActiveRef.current = true;
      setIsListening(true);
      setError(null);

      // Reuse existing stream or get new one
      let stream = audioStreamRef.current;
      if (!stream || !stream.active) {
        stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });
        audioStreamRef.current = stream;
        console.log('‚úÖ New microphone stream');
      } else {
        console.log('‚ôªÔ∏è Reusing existing stream');
      }

      // Setup new MediaRecorder
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm';
      
      const mediaRecorder = new MediaRecorder(stream, { mimeType });
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const processingType = processingTypeRef.current;
        
        if (!processingType) {
          resumeListening();
          return;
        }

        if (audioChunksRef.current.length === 0) {
          resumeListening();
          return;
        }

        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType });

        if (audioBlob.size < 5000) {
          resumeListening();
          return;
        }

        if (processingType === 'speech') {
          await handleSpeech(audioBlob);
        } else if (processingType === 'silence') {
          resumeListening();
        }
      };

      mediaRecorder.start();
      console.log('üéôÔ∏è Recording resumed');

      // Setup audio analysis - always create fresh context after TTS to avoid conflicts
        setupAudioAnalysis(stream);

    } catch (error) {
      console.error('‚ùå Resume listening error:', error);

      // Handle specific errors
      if (error.name === 'AbortError') {
        console.warn('‚ö†Ô∏è Audio operation was aborted, retrying in 1 second...');
        setTimeout(() => {
          if (isActiveRef.current) {
            resumeListening();
          }
        }, 1000);
        return;
      }

      setError('–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É');
      isActiveRef.current = false;
      setIsListening(false);
    }
  };

  // Restart listening (full reset with recalibration)
  const restartListening = () => {
    console.log('üîÑ Restarting listening...');

    // Reset all detection state
    speechFramesRef.current = 0;
    silenceFramesRef.current = 0;
    silenceAfterSpeechRef.current = 0;
    speechDetectedRef.current = false;
    processingTypeRef.current = null;
    
    // Reset noise calibration (full calibration)
    isCalibrationDoneRef.current = false;
    calibrationSamplesRef.current = [];
    noiseFloorRef.current = 0;
    isQuickCalibrationRef.current = false; // Full calibration
    
    setTimeout(() => startListening(), 1500);
  };

  // Check if Web Speech API is available
  const isWebSpeechSupported = (): boolean => {
    return !!(
      window.SpeechRecognition ||
      (window as any).webkitSpeechRecognition ||
      (window as any).mozSpeechRecognition ||
      (window as any).msSpeechRecognition
    );
  };

  // Check if lesson requires OpenAI (English, Chinese, Arabic)
  const shouldUseOpenAI = (): boolean => {
    const lessonContext = lessonContextRef.current;
    if (!lessonContext) return false;

    const title = lessonContext.title.toLowerCase();
    const description = lessonContext.description?.toLowerCase() || '';

    // Always use OpenAI for these languages
    return (
      title.includes('english') || title.includes('–∞–Ω–≥–ª–∏–π—Å–∫–∏–π') ||
      title.includes('–∞–Ω–≥–ª.') || description.includes('english') ||
      title.includes('–∫–∏—Ç–∞–π—Å–∫–∏–π') || title.includes('chinese') ||
      title.includes('–∞—Ä–∞–±—Å–∫–∏–π') || title.includes('arabic') ||
      title.includes('arab.')
    );
  };

  // Transcribe audio with smart method selection
  const transcribeAudio = async (audioBlob: Blob): Promise<string> => {
    // Determine which transcription method to use
    const useOpenAI = shouldUseOpenAI();
    const webSpeechAvailable = isWebSpeechSupported();

    console.log('üé§ Transcription method selection:');
    console.log('  - OpenAI required:', useOpenAI);
    console.log('  - Web Speech available:', webSpeechAvailable);

    // Always use OpenAI for English, Chinese, Arabic
    if (useOpenAI) {
      console.log('üéØ Using OpenAI Whisper (required for this language)');
      return transcribeWithOpenAI(audioBlob);
    }

    // For other languages, try Web Speech API first, then OpenAI fallback
    if (webSpeechAvailable) {
      try {
        console.log('üéØ Trying Web Speech API first...');
        return await transcribeWithWebSpeech(audioBlob);
      } catch (webSpeechError) {
        console.log('‚ö†Ô∏è Web Speech API failed, falling back to OpenAI:', webSpeechError.message);
        return transcribeWithOpenAI(audioBlob);
      }
    } else {
      console.log('üéØ Web Speech API not available, using OpenAI');
      return transcribeWithOpenAI(audioBlob);
    }
  };

  // Transcribe with Web Speech API (client-side)
  const transcribeWithWebSpeech = async (audioBlob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      // Check if Web Speech API is available
      const SpeechRecognition = window.SpeechRecognition || (window as any).webkitSpeechRecognition;

      if (!SpeechRecognition) {
        reject(new Error('Web Speech API not supported'));
        return;
      }

      // Web Speech API works with live microphone, not recorded audio
      // For now, we'll create a simple implementation that simulates recognition
      // TODO: Implement full Web Speech API integration for live recognition

      // For recorded audio, we'll use a timeout to simulate processing
      // and return a placeholder result
      setTimeout(() => {
        console.log('üé§ Web Speech API simulation for recorded audio');
        // In a real implementation, this would process the audioBlob
        // For now, we'll reject to use OpenAI fallback
        reject(new Error('Web Speech API requires live microphone access, using OpenAI fallback'));
      }, 100);
    });
  };

  // Transcribe with OpenAI Whisper
  const transcribeWithOpenAI = async (audioBlob: Blob): Promise<string> => {
      const formData = new FormData();
      formData.append('file', audioBlob, 'audio.webm');
      formData.append('model', 'whisper-1');

      // Determine language based on lesson context
      const lessonContext = lessonContextRef.current;
      let language = 'ru'; // Default to Russian

      if (lessonContext) {
        const title = lessonContext.title.toLowerCase();
        const description = lessonContext.description?.toLowerCase() || '';

        // Check if it's an English lesson
        if (title.includes('english') || title.includes('–∞–Ω–≥–ª–∏–π—Å–∫–∏–π') ||
            title.includes('–∞–Ω–≥–ª.') || description.includes('english')) {
          language = 'en';
          console.log('üåç Detected English lesson, using language: en');
        } else if (title.includes('–∫–∏—Ç–∞–π—Å–∫–∏–π') || title.includes('chinese')) {
          language = 'zh';
          console.log('üåç Detected Chinese lesson, using language: zh');
      } else if (title.includes('–∞—Ä–∞–±—Å–∫–∏–π') || title.includes('arabic') ||
                 title.includes('arab.')) {
        language = 'ar';
        console.log('üåç Detected Arabic lesson, using language: ar');
        } else {
          console.log('üåç Using default language: ru');
        }
      }

      formData.append('language', language);

      console.log('üé§ Sending transcription request to server...');

      const response = await fetch('/api/audio/transcriptions', {
        method: 'POST',
        body: formData
      });

      if (!response.ok) {
        const errorText = await response.text().catch(() => 'Unknown error');
        console.error('‚ùå Transcription request failed:', response.status, errorText);
        throw new Error(`Transcription failed: ${response.status} ${errorText}`);
      }

      const result = await response.json();
      console.log('‚úÖ Transcription result:', result.text?.substring(0, 50) + '...');
      return result.text || '';
  };

  // Clean markdown headers from response for better TTS and display
  const cleanMarkdownHeaders = (text: string): string => {
    // Remove headers like ## –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, ## –†–∞–∑–º–∏–Ω–∫–∞, ## –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å, etc.
    // Also remove the empty lines that follow headers
    return text
      .replace(/^## .*$/gm, '') // Remove header lines
      .replace(/^\s*$/gm, '') // Remove empty lines
      .replace(/\n\s*\n/g, '\n') // Replace multiple newlines with single
      .trim();
  };

  // Extract key theses from LLM response - ONLY facts, definitions, and key educational points
  const extractTheses = (response: string): string[] => {
    const theses: string[] = [];
    
    // Clean the response
    const teacherResponse = response.trim();

    // Split into sentences (only by periods, not by ? or !)
    // This helps avoid extracting questions
    const sentences = teacherResponse
      .split(/(?<=[.!])\s+/)
      .filter(s => s.trim().length > 15);

    // Patterns that indicate DEFINITIONS and FACTS (high priority)
    const definitionPatterns = [
      /—ç—Ç–æ\s+.{10,}/i,                          // "X - —ç—Ç–æ Y"
      /–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è?\s+.{5,}/i,                   // "–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è X"
      /—è–≤–ª—è–µ—Ç—Å—è\s+.{5,}/i,                      // "—è–≤–ª—è–µ—Ç—Å—è X"
      /–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç\s+—Å–æ–±–æ–π\s+.{5,}/i,         // "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π X"
      /—Å–æ—Å—Ç–æ–∏—Ç\s+–∏–∑\s+.{5,}/i,                  // "—Å–æ—Å—Ç–æ–∏—Ç –∏–∑ X"
      /–≤–∫–ª—é—á–∞–µ—Ç\s+.{5,}/i,                      // "–≤–∫–ª—é—á–∞–µ—Ç X"
      /–¥–µ–ª–∏—Ç—Å—è\s+–Ω–∞\s+.{5,}/i,                  // "–¥–µ–ª–∏—Ç—Å—è –Ω–∞ X"
      /–∏–º–µ–µ—Ç\s+.{5,}/i,                         // "–∏–º–µ–µ—Ç X"
      /—Å–æ–¥–µ—Ä–∂–∏—Ç\s+.{5,}/i,                      // "—Å–æ–¥–µ—Ä–∂–∏—Ç X"
      /—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω[–∞]?\s+.{5,}/i,               // "—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω X"
      /–Ω–∞—Ö–æ–¥–∏—Ç—Å—è\s+.{5,}/i,                     // "–Ω–∞—Ö–æ–¥–∏—Ç—Å—è X"
      /–æ–º—ã–≤–∞–µ—Ç—Å—è\s+.{5,}/i,                     // "–æ–º—ã–≤–∞–µ—Ç—Å—è X"
      /–≥—Ä–∞–Ω–∏—á–∏—Ç\s+—Å\s+.{5,}/i,                  // "–≥—Ä–∞–Ω–∏—á–∏—Ç —Å X"
      /–∑–∞–Ω–∏–º–∞–µ—Ç\s+.{5,}/i,                      // "–∑–∞–Ω–∏–º–∞–µ—Ç X"
      /—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç\s+.{5,}/i,                    // "—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç X"
      /—Ä–∞–≤–µ–Ω[–∞]?\s+.{3,}/i,                     // "—Ä–∞–≤–µ–Ω X"
      /–Ω–∞—Å—á–∏—Ç—ã–≤–∞–µ—Ç\s+.{3,}/i,                   // "–Ω–∞—Å—á–∏—Ç—ã–≤–∞–µ—Ç X"
      /—Å—É—â–µ—Å—Ç–≤—É–µ—Ç\s+.{5,}/i,                    // "—Å—É—â–µ—Å—Ç–≤—É–µ—Ç X"
      /–æ–±—Ä–∞–∑—É–µ—Ç—Å—è\s+.{5,}/i,                    // "–æ–±—Ä–∞–∑—É–µ—Ç—Å—è X"
      /—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è\s+.{5,}/i,                   // "—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è X"
      /–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç\s+.{5,}/i,                    // "–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç X"
    ];

    // Phrases to SKIP (questions, conversational, prompts)
    const skipPatterns = [
      /\?/,                                      // Any question
      /—Å–ª—ã—à–∞–ª\s+–ª–∏/i,                           // "—Å–ª—ã—à–∞–ª –ª–∏"
      /–∑–Ω–∞–µ—à—å\s+–ª–∏/i,                           // "–∑–Ω–∞–µ—à—å –ª–∏"
      /–º–æ–∂–µ—à—å\s+–ª–∏/i,                           // "–º–æ–∂–µ—à—å –ª–∏"
      /—Ö–æ—á–µ—à—å\s+–ª–∏/i,                           // "—Ö–æ—á–µ—à—å –ª–∏"
      /^–¥–∞–≤–∞–π/i,                                 // "–¥–∞–≤–∞–π..."
      /^—Ö–æ—Ä–æ—à–æ/i,                               // "—Ö–æ—Ä–æ—à–æ..."
      /^–æ—Ç–ª–∏—á–Ω–æ/i,                              // "–æ—Ç–ª–∏—á–Ω–æ..."
      /^–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ/i,                            // "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ..."
      /^–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ/i,                         // "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ..."
      /^–ø—Ä–∏–≤–µ—Ç/i,                               // "–ø—Ä–∏–≤–µ—Ç..."
      /^–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π/i,                           // "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π..."
      /–º–µ–Ω—è\s+–∑–æ–≤—É—Ç/i,                          // "–º–µ–Ω—è –∑–æ–≤—É—Ç..."
      /—è\s+(—é–ª—è|—é–ª–∏—è|—Ç–≤–æ–π|–≤–∞—à)/i,              // "—è –Æ–ª—è/—Ç–≤–æ–π —É—á–∏—Ç–µ–ª—å"
      /–≤—ã–±–µ—Ä–∏/i,                                // "–≤—ã–±–µ—Ä–∏..."
      /—Å–∫–∞–∂–∏/i,                                 // "—Å–∫–∞–∂–∏..."
      /–Ω–∞–ø–∏—à–∏/i,                                // "–Ω–∞–ø–∏—à–∏..."
      /—Ä–∞—Å—Å–∫–∞–∂–∏/i,                              // "—Ä–∞—Å—Å–∫–∞–∂–∏..."
      /–ø–æ–ø—Ä–æ–±—É–π/i,                              // "–ø–æ–ø—Ä–æ–±—É–π..."
      /–ø–æ–¥—É–º–∞–π/i,                               // "–ø–æ–¥—É–º–∞–π..."
      /–∫–æ–Ω–µ—á–Ω–æ/i,                               // "–∫–æ–Ω–µ—á–Ω–æ..."
      /—Ä–∞–∑—É–º–µ–µ—Ç—Å—è/i,                            // "—Ä–∞–∑—É–º–µ–µ—Ç—Å—è..."
      /–≥–æ—Ç–æ–≤[–∞]?\s+–Ω–∞—á–∞—Ç—å/i,                   // "–≥–æ—Ç–æ–≤ –Ω–∞—á–∞—Ç—å"
      /—Ä–∞–¥[–∞]?\s+—á—Ç–æ/i,                         // "—Ä–∞–¥–∞ —á—Ç–æ"
      /–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ/i,                             // "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ..."
      /–∫–∞–∫\s+—Ç—ã\s+–¥—É–º–∞–µ—à—å/i,                   // "–∫–∞–∫ —Ç—ã –¥—É–º–∞–µ—à—å"
      /—á—Ç–æ\s+—Ç—ã\s+–∑–Ω–∞–µ—à—å/i,                    // "—á—Ç–æ —Ç—ã –∑–Ω–∞–µ—à—å"
      /–Ω–∞–ø—Ä–∏–º–µ—Ä/i,                              // "–Ω–∞–ø—Ä–∏–º–µ—Ä" at start often leads to examples, not definitions
      /^–µ—Å–ª–∏/i,                                 // "–µ—Å–ª–∏..."
      /^–∫–æ–≥–¥–∞/i,                                // "–∫–æ–≥–¥–∞..."
    ];

    // First pass: find sentences with DEFINITIONS (highest quality theses)
    for (const sentence of sentences) {
      const trimmed = sentence.trim();
      if (trimmed.length < 30 || trimmed.length > 200) continue;

      // Skip if matches any skip pattern
      const shouldSkip = skipPatterns.some(pattern => pattern.test(trimmed));
      if (shouldSkip) continue;

      // Check if it's a definition
      const isDefinition = definitionPatterns.some(pattern => pattern.test(trimmed));

      if (isDefinition && theses.length < 3) {
        let cleanSentence = trimmed
          .replace(/^[*‚Ä¢-]\s*/, '')
          .replace(/\*\*([^*]+)\*\*/g, '$1')
          .replace(/^[""''""]|[""''""]$/g, '')
          .replace(/^\d+\.\s*/, '')
          .trim();

        // Remove trailing punctuation if needed
        cleanSentence = cleanSentence.replace(/[.!]+$/, '').trim();

        if (cleanSentence.length >= 25 && !theses.includes(cleanSentence)) {
          theses.push(cleanSentence);
        }
      }
    }

    // Second pass: look for numbered/bulleted educational facts
    if (theses.length < 3) {
      const listItems = teacherResponse.match(/(?:\d+\.|\*\s*|-)\s*([^.!?\n]{20,})/gi) || [];

      for (const listItem of listItems) {
        if (theses.length >= 3) break;

        const cleanItem = listItem.replace(/^\d+\.|\*\s*|-/, '').trim();
        if (cleanItem.length < 25 || cleanItem.length > 150) continue;

        // Skip if matches any skip pattern
        const shouldSkip = skipPatterns.some(pattern => pattern.test(cleanItem));
        if (shouldSkip) continue;

        // Check if it contains educational content
        const hasDefinition = definitionPatterns.some(pattern => pattern.test(cleanItem));

        if (hasDefinition && !theses.includes(cleanItem)) {
          theses.push(cleanItem.replace(/[.!]+$/, '').trim());
        }
      }
    }

    // Third pass: extract any remaining factual statements
    if (theses.length < 3) {
      // Look for sentences with numbers/statistics (often factual)
      const factualPatterns = [
        /\d+\s*(–∫–º|–º|–º–ª–Ω|—Ç—ã—Å|–ø—Ä–æ—Ü–µ–Ω—Ç|%|–≥—Ä–∞–¥—É—Å|–≥–æ–¥|–≤–µ–∫|–ª–µ—Ç)/i,
        /—Å–∞–º—ã–π\s+(–±–æ–ª—å—à–æ–π|–º–∞–ª–µ–Ω—å–∫–∏–π|–≤—ã—Å–æ–∫–∏–π|–Ω–∏–∑–∫–∏–π|–¥–ª–∏–Ω–Ω—ã–π|–∫–æ—Ä–æ—Ç–∫–∏–π|–≥–ª—É–±–æ–∫–∏–π)/i,
        /–∫—Ä—É–ø–Ω–µ–π—à–∏–π|–≤–∞–∂–Ω–µ–π—à–∏–π|–≥–ª–∞–≤–Ω—ã–π|–æ—Å–Ω–æ–≤–Ω–æ–π/i,
      ];

      for (const sentence of sentences) {
        if (theses.length >= 3) break;

        const trimmed = sentence.trim();
        if (trimmed.length < 25 || trimmed.length > 150) continue;

        const shouldSkip = skipPatterns.some(pattern => pattern.test(trimmed));
        if (shouldSkip) continue;

        const isFactual = factualPatterns.some(pattern => pattern.test(trimmed));

        if (isFactual) {
          const cleanSentence = trimmed
            .replace(/^[*‚Ä¢-]\s*/, '')
            .replace(/\*\*([^*]+)\*\*/g, '$1')
            .replace(/^\d+\.\s*/, '')
            .replace(/[.!]+$/, '')
            .trim();

          if (cleanSentence.length >= 25 && !theses.includes(cleanSentence)) {
            theses.push(cleanSentence);
          }
        }
      }
    }

    return theses.slice(0, 3);
  };

  // Get LLM response using GPT-5.1
  const getLLMResponse = async (userMessage: string): Promise<string> => {
    try {
      // Ensure we have at least basic course context
      if (!courseIdFromParams) {
        console.warn('‚ö†Ô∏è No courseId available for LLM response');
        return '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫—É—Ä—Å –¥–ª—è —É—Ä–æ–∫–∞.';
      }

      // Use current context by default, will be updated if direct load succeeds
      let effectiveLLMContext = llmContext;

    // Build system prompt like in Chat.tsx
    const buildSystemPrompt = () => {
      // –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫—É—Ä—Å–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
      const { subject, level } = parseCourseId(courseIdFromParams || 'general');
      const courseTitle = getFullCourseTitle(courseIdFromParams || 'general', level);
      const courseConfig = getCourseById(subject);

      // –î–ª—è —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∫—É—Ä—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞
      let subjectName = courseConfig?.title || subject;
      if (courseIdFromParams?.startsWith('–ï–ì–≠-') || courseIdFromParams?.startsWith('–û–ì–≠-')) {
        const examType = courseIdFromParams.startsWith('–ï–ì–≠-') ? '–ï–ì–≠' : '–û–ì–≠';
        subjectName = `${courseConfig?.title || subject} ${examType}`;
      }

      // –§–æ—Ä–º–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫—É—Ä—Å–∞
      const basePrompt = `–¢—ã - –Æ–ª—è, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —à–∫–æ–ª—å–Ω—ã–π —É—á–∏—Ç–µ–ª—å —Å 15-–ª–µ—Ç–Ω–∏–º —Å—Ç–∞–∂–µ–º.

üìö –¢–í–û–ô –¢–ï–ö–£–©–ò–ô –ö–£–†–°: "${courseTitle}"

–¢–≤–æ—è –≥–ª–∞–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ - –ø–æ–º–æ–≥–∞—Ç—å —É—á–µ–Ω–∏–∫—É –ø–æ –ø—Ä–µ–¥–º–µ—Ç—É "${subjectName}".
–¢—ã –¥–æ–ª–∂–Ω–∞:
- –û—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —É—á–µ–Ω–∏–∫–∞ –ø–æ —Ç–µ–º–∞–º —ç—Ç–æ–≥–æ –∫—É—Ä—Å–∞
- –û–±—ä—è—Å–Ω—è—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª –ø—Ä–æ—Å—Ç—ã–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º —è–∑—ã–∫–æ–º
- –ó–∞–¥–∞–≤–∞—Ç—å –¥–æ–º–∞—à–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ –∫—É—Ä—Å–∞
- –ü–æ–º–æ–≥–∞—Ç—å —Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –¥–æ–º–∞—à–Ω–∏—Ö –∑–∞–¥–∞–Ω–∏–π
- –í—ã—è–≤–ª—è—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ç–µ–º—ã –∏ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–¥ –Ω–∏–º–∏

–ï—Å–ª–∏ —É—á–µ–Ω–∏–∫ —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ —Ç–µ–º–µ, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –≤—Ö–æ–¥–∏—Ç –≤ –ø—Ä–æ–≥—Ä–∞–º–º—É "${courseTitle}",
–æ–±—ä—è—Å–Ω–∏, —á—Ç–æ —ç—Ç–∞ —Ç–µ–º–∞ –∏–∑—É—á–∞–µ—Ç—Å—è –Ω–∞ –¥—Ä—É–≥–∏—Ö —É—Ä–æ–≤–Ω—è—Ö, –Ω–æ —Ç—ã –º–æ–∂–µ—à—å –¥–∞—Ç—å –±–∞–∑–æ–≤–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ.`;

      // –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ—Ñ–∏–ª—è –æ–±—É—á–µ–Ω–∏—è
      let profileContext = '';
      if (effectiveLLMContext?.learningProfile) {
        const lp = effectiveLLMContext.learningProfile;

        profileContext += '\n\nüë§ –ü–†–û–§–ò–õ–¨ –£–ß–ï–ù–ò–ö–ê –ü–û –≠–¢–û–ú–£ –ö–£–†–°–£:';

        if (lp.weakTopics && lp.weakTopics.length > 0) {
          const unresolvedWeakTopics = lp.weakTopics.filter((t: any) => !t.resolved);
          if (unresolvedWeakTopics.length > 0) {
            profileContext += `\n‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ù–´–ï –¢–ï–ú–´ (—É–¥–µ–ª—è–π –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ):`;
            unresolvedWeakTopics.forEach((t: any) => {
              profileContext += `\n  - ${t.topic}${t.details ? `: ${t.details}` : ''}`;
            });
          }
        }

        if (lp.strongTopics && lp.strongTopics.length > 0) {
          profileContext += `\n‚úÖ –°–ò–õ–¨–ù–´–ï –°–¢–û–†–û–ù–´:`;
          lp.strongTopics.forEach((t: any) => {
            profileContext += `\n  - ${t.topic} (${t.masteryLevel}%)`;
          });
        }

        if (lp.currentHomework && lp.currentHomeworkStatus === 'pending') {
          profileContext += `\nüìù –¢–ï–ö–£–©–ï–ï –î–û–ú–ê–®–ù–ï–ï –ó–ê–î–ê–ù–ò–ï: ${lp.currentHomework}`;
          profileContext += `\n   (–ù–∞–ø–æ–º–Ω–∏ —É—á–µ–Ω–∏–∫—É –æ –î–ó, –µ—Å–ª–∏ –æ–Ω –Ω–µ –≤—ã–ø–æ–ª–Ω–∏–ª)`;
        }

        if (lp.learningPace) {
          const paceMap: Record<string, string> = {
            slow: '–º–µ–¥–ª–µ–Ω–Ω—ã–π - –æ–±—ä—è—Å–Ω—è–π –ø–æ–¥—Ä–æ–±–Ω–µ–µ –∏ –¥–∞–≤–∞–π –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤',
            normal: '–Ω–æ—Ä–º–∞–ª—å–Ω—ã–π',
            fast: '–±—ã—Å—Ç—Ä—ã–π - –º–æ–∂–Ω–æ –¥–∞–≤–∞—Ç—å –±–æ–ª—å—à–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞'
          };
          profileContext += `\nüìä –¢–µ–º–ø –æ–±—É—á–µ–Ω–∏—è: ${paceMap[lp.learningPace] || lp.learningPace}`;
        }

        // –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–º–µ—Ç–∫–∏ —É—á–∏—Ç–µ–ª—è
        if (lp.recentTeacherNotes && lp.recentTeacherNotes.length > 0) {
          profileContext += `\nüìã –ó–ê–ú–ï–¢–ö–ò –ò–ó –ü–†–û–®–õ–´–• –£–†–û–ö–û–í:`;
          lp.recentTeacherNotes.slice(-3).forEach((note: any) => {
            profileContext += `\n  - ${note.note}`;
          });
        }
      }

      // Build lesson context if available
      const lessonContext = lessonContextRef.current;
      let lessonContextText = '';
      if (lessonContext) {
        lessonContextText = `\nüìñ –¢–ï–ö–£–©–ò–ô –£–†–û–ö: "${lessonContext.title}" - ${lessonContext.topic}`;
        lessonContextText += `\n–ü–ª–∞–Ω: ${lessonContext.aspects || '–ò–∑—É—á–∞–µ–º —Ç–µ–º—É —É—Ä–æ–∫–∞'}`;
      }

      return `${basePrompt}
${profileContext}
${lessonContextText}

üéØ –¢–í–û–ô –ü–û–î–•–û–î –ö –û–ë–£–ß–ï–ù–ò–Æ:
- –û–±—ä—è—Å–Ω—è–π "–Ω–∞ –ø–∞–ª—å—Ü–∞—Ö" - –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç—ã–µ –∞–Ω–∞–ª–æ–≥–∏–∏ –∏–∑ –∂–∏–∑–Ω–∏
- –†–∞–∑–±–∏–≤–∞–π —Å–ª–æ–∂–Ω—ã–µ —Ç–µ–º—ã –Ω–∞ –ø–æ–Ω—è—Ç–Ω—ã–µ —à–∞–≥–∏
- –ó–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è
- –•–≤–∞–ª–∏ –∑–∞ —É—Å–ø–µ—Ö–∏ –∏ –º—è–≥–∫–æ —É–∫–∞–∑—ã–≤–∞–π –Ω–∞ –æ—à–∏–±–∫–∏
- –ï—Å–ª–∏ –≤–∏–¥–∏—à—å –ø—Ä–æ–±–ª–µ–º—É - –¥–æ–±–∞–≤—å –µ—ë –≤ —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ç–µ–º

–£–ß–ï–ù–ò–ö –°–ö–ê–ó–ê–õ: "${userMessage}"

–û—Ç–≤–µ—Ç—å –∫–∞–∫ —É—á–∏—Ç–µ–ª—å –ø–æ –∫—É—Ä—Å—É "${courseTitle}". –ë—É–¥—å –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–π, –Ω–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π.`;
    };

    const systemPrompt = buildSystemPrompt();

    // Final check: if we still don't have course context, try to load it synchronously
    if (!effectiveLLMContext?.course && courseIdFromParams && userIdFromStorage) {
      // Attempting synchronous load...
      try {
        // Try to get context from learning profile service directly
        const directContext = await learningProfileService.getLLMContext(userIdFromStorage, courseIdFromParams);
        if (directContext?.course) {
          console.log('‚úÖ Direct context load successful');
          // Use the loaded context for this specific call
          effectiveLLMContext = directContext;
        }
      } catch (error) {
        console.warn('‚ö†Ô∏è Direct context load failed:', error);
      }
    }

    console.log('üì§ Sending to LLM with enhanced context');
    console.log('üìö Course:', effectiveLLMContext?.course?.title || courseIdFromParams);
    const currentLessonContext = lessonContextRef.current;
    if (currentLessonContext) {
      console.log('üìñ Lesson:', currentLessonContext.title, '|', currentLessonContext.topic);
    }
    console.log('üë§ Profile loaded:', !!effectiveLLMContext?.learningProfile);
    console.log('üìã Full LLM context:', {
      hasCourse: !!effectiveLLMContext?.course,
      hasProfile: !!effectiveLLMContext?.learningProfile,
      hasSystemInstructions: !!effectiveLLMContext?.systemInstructions
    });

    // Prepare messages array with conversation history
    const conversationMessages = messages.map(m => ({
      role: m.role,
      content: m.content
    }));

    const messagesForAPI = [
      { role: 'system', content: systemPrompt },
      ...conversationMessages,
      { role: 'user', content: userMessage }
    ];

    const response = await fetch('/api/chat/completions', {
        method: 'POST',
      headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
        messages: messagesForAPI,
        model: 'gpt-5.1',
        max_completion_tokens: 800,
        temperature: 0.6,
        top_p: 0.9
        })
      });

      if (!response.ok) {
      const errorText = await response.text().catch(() => 'Unknown error');
      console.error('‚ùå Voice chat LLM request failed:', response.status, errorText);
      throw new Error('Voice chat LLM failed');
    }

      const result = await response.json();
      console.log('üì• LLM API response:', JSON.stringify(result).substring(0, 300));
      
      // Extract content safely
      const content = result?.choices?.[0]?.message?.content || '';
      if (!content) {
        console.warn('‚ö†Ô∏è LLM returned empty content, result:', result);
      }
      return content;
    } catch (error) {
      console.error('‚ùå Error in getLLMResponse:', error);
      console.error('‚ùå Error message:', error.message);
      console.error('‚ùå Error stack:', error.stack);
      throw new Error('LLM failed');
    }
  };

  // Speak text with parallel sentence generation and sequential playback
  const speakText = async (text: string): Promise<void> => {
    try {
      console.log('üîä Speaking with streaming TTS:', text.substring(0, 30) + '...');
      
      // –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ speakStreaming –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
      // –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
      await OpenAITTS.speakStreaming(text, {
        voice: 'nova',
          model: 'tts-1',
        speed: 1.0
      });
      
      console.log('‚úÖ TTS streaming complete');
      setAudioBlocked(false);

    } catch (error) {
      console.error('‚ùå TTS streaming error, using fallback:', error);
      setAudioBlocked(true);

      // Fallback to browser TTS
      return new Promise((resolve) => {
        if ('speechSynthesis' in window) {
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = 'ru-RU';
          utterance.rate = 0.9;
          utterance.pitch = 1.0;

        utterance.onend = () => {
          console.log('‚úÖ Browser TTS complete');
          resolve();
        };

          utterance.onerror = (error) => {
            console.error('‚ùå Browser TTS error:', error);
            resolve();
          };

        window.speechSynthesis.speak(utterance);
        } else {
          console.warn('‚ö†Ô∏è Speech synthesis not available');
          resolve();
        }
      });
    }
  };

  // Load lesson context from DB - reload when courseId changes
  useEffect(() => {
    // Reset lesson context when courseId changes
    if (lessonContextRef.current && !courseIdFromParams) {
      lessonContextRef.current = null;
    }

    const loadLessonContext = async () => {
      try {
        console.log('üîç Checking user state for currentLesson...');
        const userState = await sessionService.getUserState();

        console.log('üì¶ Full user state:', userState);
        console.log('üéØ Expected courseId:', courseIdFromParams);
        console.log('üìã Stored courseId:', userState?.currentCourseId);

        // Check if userState contains data for the correct course
        const courseMatches = userState?.currentCourseId === courseIdFromParams || 
                               userState?.currentCourseId === String(courseIdFromParams);

        if (!courseMatches && userState?.currentCourseId) {
          console.log('‚ö†Ô∏è Course mismatch! UserState has data for:', userState?.currentCourseId, 'but we need:', courseIdFromParams);
          console.log('üßπ Clearing old lesson context data...');
          // Clear old data to force fresh lesson loading
          await sessionService.clearCourseState();
          console.log('‚úÖ Old data cleared');
          // Don't return - continue to load correct lesson
        }

        // If we have matching course data, use it
        if (courseMatches && userState?.currentLessonData) {
          const lessonData = userState.currentLessonData;
          console.log('üì¶ Raw lesson data:', lessonData);

          // Build context from currentLessonData (same structure as Chat.tsx uses)
        const context = {
          title: lessonData.title || '–£—Ä–æ–∫',
          topic: lessonData.topic || '',
            // Use aspects as the main description (this is what Chat.tsx uses for lesson content)
            description: lessonData.aspects || lessonData.description || lessonData.content || ''
        };

        lessonContextRef.current = context;
        console.log('üìö Lesson context loaded from userState:');
        console.log('  Title:', context.title);
        console.log('  Topic:', context.topic);
          console.log('  Description:', context.description?.substring(0, 200) + '...');
        } else if (courseIdFromParams) {
          // If no matching data, try to load course from API
          console.log('üì° No matching lesson data, loading course from API...');
          try {
            const courseService = (await import('@/services/courseService')).default;
            const courseData = await courseService.getCourse(courseIdFromParams);
            
            if (courseData && courseData.currentLesson) {
              const context = {
                title: courseData.currentLesson.title || '–£—Ä–æ–∫',
                topic: courseData.currentLesson.topic || '',
                description: courseData.currentLesson.aspects || courseData.currentLesson.description || courseData.currentLesson.content || ''
              };
              
              lessonContextRef.current = context;
              console.log('üìö Lesson context loaded from API:');
              console.log('  Title:', context.title);
              console.log('  Topic:', context.topic);
            } else {
              console.warn('‚ö†Ô∏è No lesson data in course from API');
            }
          } catch (error) {
            console.error('‚ùå Error loading course from API:', error);
          }
      } else {
          console.warn('‚ö†Ô∏è No lesson context found and no courseId to load from');
      }
    } catch (error) {
      console.error('‚ùå Error loading lesson context:', error);
    }
    };
    loadLessonContext();
  }, [courseIdFromParams]);

  // Mount effect
  useEffect(() => {
    // Prevent multiple initializations
    if (initializationStartedRef.current) {
      console.log('‚ö†Ô∏è Initialization already started, skipping...');
      return;
    }

    initializationStartedRef.current = true;
    console.log('üéì VoiceCallPage mounted');
    console.log('üé§ Web Speech API supported:', isWebSpeechSupported());
    console.log('üìã Course ID from params:', courseIdFromParams);
    console.log('üë§ User ID:', userIdFromStorage);

    // Force load LLM context at startup to ensure it's available
    const forceLoadContext = async () => {
      if (userIdFromStorage && courseIdFromParams) {
        console.log('üîÑ Force loading LLM context at startup...');
        try {
          await loadLLMContext();
          console.log('‚úÖ LLM context force-loaded at startup');
        } catch (error) {
          console.warn('‚ö†Ô∏è Failed to force-load LLM context:', error);
        }
      }
    };

    // Send welcome message if chat is empty and lesson context is loaded
    const initializeChat = async () => {
      // First, ensure context is loaded
      await forceLoadContext();
      console.log('üéì VoiceCallPage initializing...');
      console.log('üìã courseIdFromParams:', courseIdFromParams);
      console.log('üë§ userIdFromStorage:', userIdFromStorage);
      console.log('üîÑ isLoadingProfile:', isLoadingProfile);
      console.log('üìö lessonContextRef.current:', !!lessonContextRef.current);
      console.log('ü§ñ llmContext:', !!llmContext);

      // Wait for learning profile and lesson context to load (extended timeout)
      console.log('‚è≥ Waiting for profile and lesson context...');
      let attempts = 0;
      const maxAttempts = 50; // Increased from 20 to 50 (5 seconds total)

      // For voice lessons, we only need LLM context and course data, not lesson context
      while (attempts < maxAttempts && (isLoadingProfile || !llmContext || !llmContext?.course)) {
        await new Promise(resolve => setTimeout(resolve, 100));
        attempts++;
        // –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ 20-–π –ø–æ–ø—ã—Ç–∫–µ (–∫–∞–∂–¥—ã–µ 2 —Å–µ–∫—É–Ω–¥—ã)
        if (attempts === 20) {
          console.log('‚è≥ Loading context...');
        }
      }

      console.log('‚úÖ Wait complete - Loading:', isLoadingProfile, 'Lesson:', !!lessonContextRef.current, 'LLM Context:', !!llmContext);

      // If no learning profile loaded (but LLM context exists), try to create profile manually (only once)
      if (llmContext && !llmContext.learningProfile && userIdFromStorage && courseIdFromParams && !isLoadingProfile && !profileCreationAttemptedRef.current) {
        profileCreationAttemptedRef.current = true; // Mark as attempted
        console.log('üìã LLM context loaded but no learning profile found, creating profile manually...');
        try {
          // Create profile using the same method as chat
          await analyzeAndUpdateFromLLM('', 'system', `–ù–∞—á–∞–ª–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —É—Ä–æ–∫–∞ –ø–æ –∫—É—Ä—Å—É ${courseIdFromParams}`);

          // Wait for profile creation
          await new Promise(resolve => setTimeout(resolve, 500));

          console.log('‚úÖ Profile created, reloading LLM context...');
          // Reload LLM context instead of reloading page
          await loadLLMContext();
          console.log('‚úÖ LLM context reloaded');

        } catch (error) {
          console.warn('‚ö†Ô∏è Failed to create profile:', error);
          profileCreationAttemptedRef.current = false; // Reset on error to allow retry
        }
      }

      if (messages.length === 0) {
        console.log('üí¨ Chat is empty, sending welcome message...');
        await sendWelcomeMessage();
      }

    startListening();
    };

    initializeChat();

    return () => {
      console.log('üéì VoiceCallPage unmounting');
      cleanup();
      initializationStartedRef.current = false; // Reset for next mount
      profileCreationAttemptedRef.current = false; // Reset for next mount
    };
  }, []);

  // Avatar animation is now CSS-based, no video control needed

  return (
    <div className="min-h-screen bg-gray-50">
      <HeaderWithHero />
      <div className="container mx-auto px-4 py-8">
        <div className="max-w-4xl mx-auto">
          <Button
            variant="ghost"
            onClick={() => navigate(-1)}
            className="mb-4 flex items-center gap-2"
          >
            <ArrowLeft className="w-4 h-4" />
            –í–µ—Ä–Ω—É—Ç—å—Å—è –Ω–∞–∑–∞–¥
          </Button>

          <Card>
            <CardHeader>
              <CardTitle className="flex items-center gap-2">
                <Mic className="w-5 h-5" />
              </CardTitle>
            </CardHeader>
            <CardContent className="space-y-6">
              {/* Video Avatar */}
              <div className="text-center">
                <div className="relative inline-block">
                  {/* Animated avatar - always visible */}
                  <div 
                    className="w-48 h-48 rounded-full bg-gradient-to-br from-purple-500 via-pink-500 to-rose-500 border-4 border-gray-200 shadow-lg flex items-center justify-center text-white text-6xl font-bold relative overflow-hidden"
                  >
                    <span className="z-10">–Æ</span>
                    {/* Animated background when speaking */}
                    {isSpeaking && (
                      <div className="absolute inset-0 bg-gradient-to-br from-purple-600 via-pink-600 to-rose-600 animate-pulse" />
                    )}
                    {/* Listening indicator */}
                    {isListening && (
                      <div className="absolute inset-0 border-4 border-green-400 rounded-full animate-ping opacity-50" />
                    )}
                  </div>

                  {/* Status overlay */}
                  <div className="absolute -bottom-2 left-1/2 transform -translate-x-1/2 bg-white px-3 py-1 rounded-full shadow-md border">
                    {isListening && (
                      <div className="flex items-center gap-2 text-green-600">
                        <div className="w-2 h-2 bg-green-600 rounded-full animate-pulse"></div>
                        <span className="text-sm font-medium">–°–ª—É—à–∞–µ—Ç</span>
                  </div>
                )}
                {isProcessing && (
                      <div className="flex items-center gap-2 text-blue-600">
                        <Loader2 className="w-3 h-3 animate-spin" />
                        <span className="text-sm font-medium">–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç</span>
                  </div>
                )}
                {isSpeaking && (
                      <div className="flex items-center gap-2 text-purple-600">
                        <div className="w-2 h-2 bg-purple-600 rounded-full animate-pulse"></div>
                        <span className="text-sm font-medium">–ì–æ–≤–æ—Ä–∏—Ç</span>
                      </div>
                    )}
                  </div>

                  {/* Audio blocked indicator */}
                  {audioBlocked && (
                    <div
                      className="absolute top-2 left-1/2 transform -translate-x-1/2 bg-yellow-50 border border-yellow-200 px-3 py-2 rounded-lg shadow-md max-w-xs cursor-pointer hover:bg-yellow-100 transition-colors"
                      onClick={() => setAudioBlocked(false)}
                    >
                      <div className="text-xs text-yellow-800 text-center">
                        <div className="flex items-center justify-center gap-1 font-medium mb-1">
                          üîá –ê–≤—Ç–æ–≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ
                        </div>
                        <div className="text-xs opacity-90">
                          –ù–∞–∂–º–∏—Ç–µ –Ω–∞ –ª—é–±—É—é –∫–Ω–æ–ø–∫—É –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –∑–≤—É–∫–∞
                        </div>
                      </div>
                    </div>
                  )}
                </div>
              </div>


              {/* Control buttons */}
              <div className="flex justify-center gap-4">
                <Button
                  variant={isMuted ? "destructive" : "outline"}
                  onClick={toggleMute}
                  className="flex items-center gap-2"
                >
                  {isMuted ? <MicOff className="w-4 h-4" /> : <Mic className="w-4 h-4" />}
                  {isMuted ? '–í–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω' : '–í—ã–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω'}
                </Button>
              </div>

              {/* Key Theses - Displayed between microphone button and end lesson button */}
              {speechTheses.length > 0 && (
                <div className="bg-gradient-to-r from-blue-50 to-indigo-50 border border-blue-200 rounded-lg p-4 shadow-sm">
                  <h3 className="text-sm font-semibold text-blue-900 mb-3 flex items-center gap-2">
                    <span className="w-2 h-2 bg-blue-500 rounded-full"></span>
                    –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã —É—Ä–æ–∫–∞
                  </h3>
                  <ol className="space-y-2">
                    {speechTheses.map((thesis, index) => (
                      <li key={index} className="text-sm text-gray-700 flex items-start gap-2">
                        <span className="flex-shrink-0 w-5 h-5 rounded-full bg-blue-500 text-white text-xs flex items-center justify-center font-medium">
                          {index + 1}
                        </span>
                        <span className="flex-1">{thesis}</span>
                      </li>
                    ))}
                  </ol>
                  <button
                    onClick={() => setSpeechTheses([])}
                    className="mt-3 text-xs text-blue-600 hover:text-blue-800 underline"
                  >
                    –°–∫—Ä—ã—Ç—å
                  </button>
                </div>
              )}

              {/* End lesson button */}
              <div className="flex justify-center">
                <Button
                  variant="outline"
                  onClick={endLesson}
                  className="flex items-center gap-2 text-red-600 hover:text-red-700 hover:bg-red-50"
                >
                  <PhoneOff className="w-4 h-4" />
                  –ó–∞–≤–µ—Ä—à–∏—Ç—å —É—Ä–æ–∫
                </Button>
              </div>

              {/* Error */}
              {error && (
                <div className="p-4 bg-red-50 border border-red-200 rounded-lg text-red-700">
                  {error}
                </div>
              )}

            </CardContent>
          </Card>
        </div>
      </div>
    </div>
  );
};

export default VoiceCallPage;
